# ğŸš€ Incremental ETL Pipeline with Airflow -- Production-Style Data Architecture

## ğŸ“Œ Project Overview

This project implements a **production-style incremental ETL pipeline**,
designed following modern data engineering best practices for enterprise
environments.

The pipeline extracts data from the GitHub REST API, transforms it, and
loads it into a dimensional Data Warehouse model, incorporating:

-   Incremental loading with per-entity watermark control
-   Idempotent data loads
-   Automatic deduplication
-   Built-in Data Quality checks
-   Run-level auditing and traceability
-   Automatic failure detection with root cause logging

The goal of this project is to simulate a real-world enterprise data
pipeline where reliability, observability, and data consistency are
critical.

------------------------------------------------------------------------

# ğŸ— Architecture

## ğŸ”„ General Flow

    GitHub API
         â†“
    RAW Layer (JSON storage)
         â†“
    STAGING Layer (Normalization + Deduplication)
         â†“
    DATA WAREHOUSE (Dimensional Model)
         â†“
    Data Quality & Run Audit

------------------------------------------------------------------------

## ğŸ—‚ Data Layers

### 1ï¸âƒ£ RAW Layer

-   Full JSON payload persistence
-   Immutable ingestion
-   Enables reprocessing without additional API calls

Tables: - `raw.github_repos` - `raw.github_commits`

------------------------------------------------------------------------

### 2ï¸âƒ£ STAGING Layer

-   Data normalization
-   Deduplication using window functions (`ROW_NUMBER()`)
-   Prevents PostgreSQL cardinality violations during UPSERT

Tables: - `stg.repos` - `stg.commits`

------------------------------------------------------------------------

### 3ï¸âƒ£ Data Warehouse (Dimensional Model)

Optimized for analytics and reporting:

-   `dw.dim_repository`
-   `dw.fact_commits`

Relationship:

    fact_commits.repo_id â†’ dim_repository.repo_id

This structure enables repository-level and time-based analysis of
commit activity.

------------------------------------------------------------------------

# ğŸ” Incremental Strategy

The pipeline implements a robust incremental loading strategy based on:

-   Control table: `dw.etl_watermark`
-   Per-entity watermark tracking
-   Watermark updates only after successful FACT load
-   Protection against logical data loss

This approach prevents common production issues such as:

-   Skipping data due to intermediate failures
-   Inconsistent reprocessing
-   Premature state updates

------------------------------------------------------------------------

# ğŸ§  Key Engineering Decisions

### âœ… Idempotent Loads

The `stg.commits` table uses `ON CONFLICT` with natural key (`sha`),
ensuring:

-   Safe reprocessing
-   Consistency across repeated executions
-   Elimination of multi-batch duplicates

------------------------------------------------------------------------

### âœ… Window-Based Deduplication

Implemented using:

``` sql
ROW_NUMBER() OVER (PARTITION BY sha ORDER BY raw_ingested_at DESC)
```

This prevents errors such as:

    ON CONFLICT DO UPDATE command cannot affect row a second time

A common issue in real-world incremental pipelines.

------------------------------------------------------------------------

### âœ… Correct Watermark Handling

The watermark is updated only after:

    LOAD FACT â†’ UPDATE WATERMARK

Never during extraction.

This protects against data loss when transformations or loads fail.

------------------------------------------------------------------------

# ğŸ“Š Integrated Data Quality Checks

The pipeline includes automated validation tasks that can stop execution
if inconsistencies are detected:

-   âœ” No NULL `repo_id` values in FACT
-   âœ” Referential integrity between FACT and DIM
-   âœ” Watermark consistency validation
-   âœ” RAW â†’ STAGING propagation sanity check

If any validation fails:

-   The DAG is marked as FAILED
-   The root cause task is recorded
-   Full execution traceability is preserved

This aligns with modern Data Reliability Engineering practices.

------------------------------------------------------------------------

# ğŸ§¾ Run-Level Auditing & Observability

Table: `dw.etl_run_log`

Each execution records:

-   Run ID
-   Start and end timestamps
-   Status (RUNNING / SUCCESS / FAILED)
-   Root cause task (if failed)
-   Metrics per layer:
    -   RAW batches
    -   STAGING rows
    -   DIM rows
    -   FACT rows

This enables:

-   Historical traceability
-   Fast troubleshooting
-   Pipeline stability monitoring

------------------------------------------------------------------------

# ğŸ³ Local Execution

## Requirements

-   Docker
-   Docker Compose

## Start environment

``` bash
docker compose up -d
```

Airflow UI:

    http://localhost:8080

The DAG can be triggered manually or scheduled daily.

------------------------------------------------------------------------

# ğŸ›  Tech Stack

-   Python
-   Apache Airflow
-   PostgreSQL
-   Docker
-   GitHub REST API
-   Advanced SQL (CTEs, window functions, UPSERT)

------------------------------------------------------------------------

# ğŸ“ˆ Future Improvements

-   FACT table partitioning
-   Data freshness monitoring
-   Automated alerts (Slack/Email)
-   CI/CD integration
-   Incremental optimization by batch tracking
-   Monitoring dashboard (Power BI / Metabase)

------------------------------------------------------------------------

# ğŸ’¼ Professional Objective

This project was developed to demonstrate:

-   Robust incremental pipeline design
-   Enterprise-grade data engineering practices
-   Data governance and reliability concepts
-   Observability in ETL workflows

It simulates a real-world enterprise data environment where consistency,
reliability, and traceability are priorities.

------------------------------------------------------------------------

# ğŸ‘¨â€ğŸ’» Author

Dubey Angulo\
Systems Engineer\
Colombia ğŸ‡¨ğŸ‡´


------------------------------------------------------------------------

# ğŸ¯ Focus

This project demonstrates capabilities in:

-   Architectural thinking
-   Production-grade problem solving
-   Secure incremental data design
-   Failure management and auditing
-   Business-oriented data engineering
